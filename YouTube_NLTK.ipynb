{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Python\n",
    "\n",
    "All credentials and my thanks to this awesome guy [Sendex](https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_text = \"Hello Mr. Smith, how are you doing today? The weather is great and python is awesome. The sky is pinkish-blue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great and python is awesome.', 'The sky is pinkish-blue']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_sentence = \"This is an example showing off stop word filtration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = word_tokenize(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_sentence = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        \n",
    "# filtered_sentence = [w for w in words if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_text = \"It is very important to be pythonly while you are pythoning with python. All pythoners have pythoned badly at least once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = word_tokenize(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "be\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "All\n",
      "python\n",
      "have\n",
      "python\n",
      "badli\n",
      "at\n",
      "least\n",
      "onc\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer # unsipervised ML tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "CC: conjunction, coordinating\n",
    "    & 'n and both but either et for less minus neither nor or plus so\n",
    "    therefore times v. versus vs. whether yet\n",
    "CD: numeral, cardinal\n",
    "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
    "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
    "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
    "DT: determiner\n",
    "    all an another any both del each either every half la many much nary\n",
    "    neither no some such that the them these this those\n",
    "EX: existential there\n",
    "    there\n",
    "FW: foreign word\n",
    "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
    "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
    "    terram fiche oui corporis ...\n",
    "IN: preposition or conjunction, subordinating\n",
    "    astride among uppon whether out inside pro despite on by throughout\n",
    "    below within for towards near behind atop around if like until below\n",
    "    next into if beside ...\n",
    "JJ: adjective or numeral, ordinal\n",
    "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
    "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
    "    multilingual multi-disciplinary ...\n",
    "JJR: adjective, comparative\n",
    "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
    "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
    "    cozier creamier crunchier cuter ...\n",
    "JJS: adjective, superlative\n",
    "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
    "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
    "    dearest deepest densest dinkiest ...\n",
    "LS: list item marker\n",
    "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
    "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
    "    two\n",
    "MD: modal auxiliary\n",
    "    can cannot could couldn't dare may might must need ought shall should\n",
    "    shouldn't will would\n",
    "NN: noun, common, singular or mass\n",
    "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
    "    investment slide humour falloff slick wind hyena override subhumanity\n",
    "    machinist ...\n",
    "NNP: noun, proper, singular\n",
    "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
    "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
    "    Shannon A.K.C. Meltex Liverpool ...\n",
    "NNPS: noun, proper, plural\n",
    "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
    "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
    "    Apache Apaches Apocrypha ...\n",
    "NNS: noun, common, plural\n",
    "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
    "    divestitures storehouses designs clubs fragrances averages\n",
    "    subjectivists apprehensions muses factory-jobs ...\n",
    "PDT: pre-determiner\n",
    "    all both half many quite such sure this\n",
    "POS: genitive marker\n",
    "    ' 's\n",
    "PRP: pronoun, personal\n",
    "    hers herself him himself hisself it itself me myself one oneself ours\n",
    "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
    "PRP$: pronoun, possessive\n",
    "    her his mine my our ours their thy your\n",
    "RB: adverb\n",
    "    occasionally unabatingly maddeningly adventurously professedly\n",
    "    stirringly prominently technologically magisterially predominately\n",
    "    swiftly fiscally pitilessly ...\n",
    "RBR: adverb, comparative\n",
    "    further gloomier grander graver greater grimmer harder harsher\n",
    "    healthier heavier higher however larger later leaner lengthier less-\n",
    "    perfectly lesser lonelier longer louder lower more ...\n",
    "RBS: adverb, superlative\n",
    "    best biggest bluntest earliest farthest first furthest hardest\n",
    "    heartiest highest largest least less most nearest second tightest worst\n",
    "RP: particle\n",
    "    aboard about across along apart around aside at away back before behind\n",
    "    by crop down ever fast for forth from go high i.e. in into just later\n",
    "    low more off on open out over per pie raising start teeth that through\n",
    "    under unto up up-pp upon whole with you\n",
    "SYM: symbol\n",
    "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
    "TO: \"to\" as preposition or infinitive marker\n",
    "    to\n",
    "UH: interjection\n",
    "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
    "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
    "    man baby diddle hush sonuvabitch ...\n",
    "VB: verb, base form\n",
    "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
    "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
    "    boost brace break bring broil brush build ...\n",
    "VBD: verb, past tense\n",
    "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
    "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
    "    speculated wore appreciated contemplated ...\n",
    "VBG: verb, present participle or gerund\n",
    "    telegraphing stirring focusing angering judging stalling lactating\n",
    "    hankerin' alleging veering capping approaching traveling besieging\n",
    "    encrypting interrupting erasing wincing ...\n",
    "VBN: verb, past participle\n",
    "    multihulled dilapidated aerosolized chaired languished panelized used\n",
    "    experimented flourished imitated reunifed factored condensed sheared\n",
    "    unsettled primed dubbed desired ...\n",
    "VBP: verb, present tense, not 3rd person singular\n",
    "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
    "    appear tend stray glisten obtain comprise detest tease attract\n",
    "    emphasize mold postpone sever return wag ...\n",
    "VBZ: verb, present tense, 3rd person singular\n",
    "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
    "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
    "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
    "WDT: WH-determiner\n",
    "    that what whatever which whichever\n",
    "WP: WH-pronoun\n",
    "    that what whatever whatsoever which who whom whosoever\n",
    "WP$: WH-pronoun, possessive\n",
    "    whose\n",
    "WRB: Wh-adverb\n",
    "    how however whence whenever where whereby whereever wherein whereof why\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "\n",
    "Chunking in Natural Language Processing (NLP) is the process by which we group various words together by their part of speech tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinking\n",
    "\n",
    "A chink is what we wish to remove from the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            \n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "Named entity recognition is useful to quickly find out what the subjects of discussion are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            namedEnt.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing\n",
    "A very similar operation to stemming is called lemmatizing. The major difference between these is, as you saw earlier, stemming can often create non-existent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) # \"a\" for ajective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_text = gutenberg.raw(\"bible-kjv.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok = sent_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'[The King James Bible]\\n\\nThe Old Testament of the King James Bible\\n\\nThe First Book of Moses:  Called Genesis\\n\\n\\n1:1 In the beginning God created the heaven and the earth.', u'1:2 And the earth was without form, and void; and darkness was upon\\nthe face of the deep.', u'And the Spirit of God moved upon the face of the\\nwaters.', u'1:3 And God said, Let there be light: and there was light.', u'1:4 And God saw the light, that it was good: and God divided the light\\nfrom the darkness.']\n"
     ]
    }
   ],
   "source": [
    "print(tok[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn = wordnet.synsets(\"program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
     ]
    }
   ],
   "source": [
    "# synonims to the word\n",
    "print(syn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan\n"
     ]
    }
   ],
   "source": [
    "print(syn[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of steps to be carried out or goals to be accomplished\n"
     ]
    }
   ],
   "source": [
    "# definition\n",
    "print(syn[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'they drew up a six-step plan', u'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "print(syn[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synonims = []\n",
    "antonims = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonims.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonims.append(l.antonyms()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'beneficial', u'right', u'secure', u'just', u'unspoilt', u'respectable', u'good', u'goodness', u'dear', u'salutary', u'ripe', u'expert', u'skillful', u'in_force', u'proficient', u'unspoiled', u'dependable', u'soundly', u'honorable', u'full', u'undecomposed', u'safe', u'adept', u'upright', u'trade_good', u'sound', u'in_effect', u'practiced', u'effective', u'commodity', u'estimable', u'well', u'honest', u'near', u'skilful', u'thoroughly', u'serious'])\n",
      "================\n",
      "set([u'bad', u'badness', u'ill', u'evil', u'evilness'])\n"
     ]
    }
   ],
   "source": [
    "print(set(synonims))\n",
    "print(\"================\")\n",
    "print(set(antonims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2 = wordnet.synset(\"boat.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w3 = wordnet.synset(\"cat.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909090909091\n"
     ]
    }
   ],
   "source": [
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "source": [
    "print(w1.wup_similarity(w3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695652173913\n"
     ]
    }
   ],
   "source": [
    "print(w1.wup_similarity(wordnet.synset(\"car.n.01\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
